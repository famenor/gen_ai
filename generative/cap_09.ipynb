{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6582cb35-b8e3-4dd4-a90f-8e2bfe0d679d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ae9bbb0-ef44-4aa8-af73-0dc966e1c195",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af68e6f6-85ba-44d1-8450-2a9bd0147cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41b375d4-ccd5-4877-9715-774d80a31d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS = 4096\n",
    "\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 25\n",
    "\n",
    "# Estimate the number of tokens that will be charged for during training\n",
    "def estimate_tokens(dataset, assistant_tokens):\n",
    "    # Set the initial number of epochs to the target epochs\n",
    "    n_epochs = TARGET_EPOCHS\n",
    "\n",
    "    # Get the number of examples in the dataset\n",
    "    n_train_examples = len(dataset)\n",
    "\n",
    "    # If the examples total is less than the minimum target\n",
    "    # adjust the epochs to ensure we have enough examples for\n",
    "    # training\n",
    "    if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "        n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "    \n",
    "    # If the  number of examples is more than the maximum target\n",
    "    # adjust the  epochs to ensure we don't exceed the maximum \n",
    "    # for training\n",
    "    elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "        n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "    # Calculate the total number of tokens in the dataset\n",
    "    n_billing_tokens_in_dataset = sum(min(MAX_TOKENS, length) for length in assistant_tokens)\n",
    "\n",
    "    # Print the total token count that will be charged during training\n",
    "    print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "\n",
    "    # Print the default number of epochs for training\n",
    "    print(f\"You will train for {n_epochs} epochs on this dataset\")\n",
    "\n",
    "    # Print the total number of tokens that will be charged during training\n",
    "    print(f\"You will be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
    "\n",
    "    # If the total token count exceeds the maximum tokens, print a warning \n",
    "    if n_billing_tokens_in_dataset > MAX_TOKENS:\n",
    "        print(f\"WARNING: Your dataset contains examples longer than 4K tokens by {n_billing_tokens_in_dataset - MAX_TOKENS} tokens.\")\n",
    "        print(\"You will be charged for the full length of these examples during training, but only the first 4K tokens will be used for training.\")\n",
    "\n",
    "# Print the number of tokens in the messages\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "# print the number of tokens in the assistant messages\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "# Print the distribution of values\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ade97ea-71f0-4196-918e-819e422be3bc",
   "metadata": {},
   "source": [
    "# Leer JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4ef2860-d9f8-4694-bb21-fe2544315a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = None\n",
    "with open('emojis.jsonl', 'r', encoding='utf-8') as f:\n",
    "    dataset = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1feb3811-9359-4ccd-8509-892d7b28c1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'role': 'system', 'content': \"You're a chatbot that only responds with emojis!\"}, {'role': 'user', 'content': 'I just passed my driving test!'}, {'role': 'assistant', 'content': '(party)'}]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0276019b-a6bc-41c1-b1bf-192d2feee4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Distribution of total tokens:\n",
      "min / max: 31, 50\n",
      "mean / median: 36.838312829525485, 37.0\n",
      "p5 / p95: 34.0, 40.0\n",
      "\n",
      "#### Distribution of assistant tokens:\n",
      "min / max: 2, 10\n",
      "mean / median: 4.050966608084359, 4.0\n",
      "p5 / p95: 3.0, 6.0\n",
      "Dataset has ~2305 tokens that will be charged for during training\n",
      "You will train for 3 epochs on this dataset\n",
      "You will be charged for ~6915 tokens\n",
      "Processing file completed: emojis.jsonl\n"
     ]
    }
   ],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "file = 'emojis.jsonl'\n",
    "with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "total_tokens = []\n",
    "assistant_tokens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex.get(\"messages\", {})\n",
    "    total_tokens.append(num_tokens_from_messages(messages))\n",
    "    assistant_tokens.append(num_assistant_tokens_from_messages(messages))\n",
    "\n",
    "print_distribution(total_tokens, \"total tokens\")\n",
    "print_distribution(assistant_tokens, \"assistant tokens\")\n",
    "estimate_tokens(dataset, assistant_tokens)\n",
    "print(f\"Processing file completed: {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1220bf-c1a3-4360-b2f8-f0be9932f8a0",
   "metadata": {},
   "source": [
    "# Upload and finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53c4ad2b-fb03-4f83-bd16-25759038e7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file ID: file-HGB2NrCGDueg6xhv6asQX1\n",
      "Training file name: emojis.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Upload the training and validation dataset files to Azure OpenAI with the SDK.\n",
    "file = client.files.create(file=open(file, \"rb\"), purpose=\"fine-tune\")\n",
    "\n",
    "print(\"Training file ID:\", file.id)\n",
    "print(\"Training file name:\", file.filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a47be4e-fceb-47e5-8726-2abc81f2ef1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuning job ID: ftjob-EzA3GXhVl4S4js5BDwcKo194\n"
     ]
    }
   ],
   "source": [
    "API_VERSION = '2023-09-15-preview'\n",
    "\n",
    "ft = client.fine_tuning.jobs.create(\n",
    "    training_file=\"file-HGB2NrCGDueg6xhv6asQX1\",\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    hyperparameters={\n",
    "        \"n_epochs\":3\n",
    "    },\n",
    "    suffix=\"emoji\"\n",
    ")\n",
    "print(\"Finetuning job ID:\", ft.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2cb101b9-0fa4-40cb-9fa1-ea2042ac4b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ftjob-EzA3GXhVl4S4js5BDwcKo194 succeeded\n"
     ]
    }
   ],
   "source": [
    "# List all the FT jobs\n",
    "ft_jobs = client.fine_tuning.jobs.list()\n",
    "\n",
    "for ft_job in ft_jobs:\n",
    "    print(ft_job.id, ft_job.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712fdc6d-72c1-4189-9794-19636d1e4ccf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
